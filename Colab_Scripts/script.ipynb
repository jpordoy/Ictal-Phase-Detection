{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import re\n",
    "from scipy import stats\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from keras import layers, regularizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, MaxPooling1D,  Input, Bidirectional, Conv1D, concatenate, Permute\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "\n",
    "from tensorflow.keras.layers import Concatenate, Add, GlobalMaxPooling1D, Attention,Activation\n",
    "from keras import regularizers\n",
    "from keras import layers, regularizers\n",
    "\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, Activation, MaxPooling1D, LSTM, Bidirectional, Permute, Concatenate, Attention, Input, Dense, Dropout, Multiply, Reshape\n",
    "from tensorflow.keras import regularizers, Model\n",
    "from tensorflow.keras.layers import Concatenate, Add, GlobalMaxPooling1D, Attention,Activation\n",
    "from keras import regularizers\n",
    "from keras import layers, regularizers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Globals\n",
    "batch_size = 8\n",
    "num_classes = 3  # classes, seizure/no seizure\n",
    "epochs = 25      # Epoch iterations\n",
    "row_hidden = 128  # hidden neurons in conv layers\n",
    "col_hidden = 128   # hidden neurons in the Bi-LSTM layers\n",
    "RANDOM_SEED = 333    \n",
    "N_TIME_STEPS = 125   # 50 records in each sequence\n",
    "N_FEATURES = 2      # mag,hr,roi_Ratio,output\n",
    "step = 100           # window overlap = 50 -10 = 40  (80% overlap)\n",
    "N_CLASSES = 3       # class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventID</th>\n",
       "      <th>hr</th>\n",
       "      <th>rawData</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5635</td>\n",
       "      <td>75</td>\n",
       "      <td>426.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5635</td>\n",
       "      <td>75</td>\n",
       "      <td>2267.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5635</td>\n",
       "      <td>75</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5635</td>\n",
       "      <td>75</td>\n",
       "      <td>4090.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5635</td>\n",
       "      <td>75</td>\n",
       "      <td>3057.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eventID  hr  rawData  label\n",
       "0     5635  75    426.0      1\n",
       "1     5635  75   2267.0      1\n",
       "2     5635  75   2002.0      1\n",
       "3     5635  75   4090.0      1\n",
       "4     5635  75   3057.0      1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading data from google drive /content/gdrive/MyDrive/OSD Research/data/seizures.csv\n",
    "mypath = '../Data/Train.csv'\n",
    "df = pd.read_csv(mypath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Number of data points in class 1 = 1601.0 ~ 40.54%\n",
      "Number of data points in class 0 = 1511.0 ~ 38.26%\n",
      "Number of data points in class 2 = 837.0 ~ 21.2%\n",
      "--------------------------------------------------\n",
      "total datapoints: 3949.0\n"
     ]
    }
   ],
   "source": [
    "# analysis of class labels\n",
    "\n",
    "label_dict = dict(df['label'].value_counts()/125)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize = (8, 4))\n",
    "sns.barplot(x = list(label_dict.keys()), y = list(label_dict.values()))\n",
    "plt.xlabel('Activity')\n",
    "plt.ylabel('Data points per activity')\n",
    "plt.title('Number of samples by activities')\n",
    "plt.show()\n",
    "\n",
    "# Percentage-wise distribution of the class label yi's\n",
    "print(\"- \"* 50)\n",
    "for i in label_dict.keys():\n",
    "  print(\"Number of data points in class {0} = {1} ~ {2}%\".format(\n",
    "  i, label_dict[i], round((label_dict[i]*100)/sum(label_dict.values()), 2)))\n",
    "\n",
    "\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"total datapoints:\", sum(label_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "RANDOM_SEED = 333    \n",
    "N_TIME_STEPS = 125   # 50 records in each sequence\n",
    "N_FEATURES = 2      # mag,hr,roi_Ratio,output\n",
    "step = 100           # window overlap = 50 -10 = 40  (80% overlap)\n",
    "N_CLASSES = 3       # class labels\n",
    "\n",
    "segments = []\n",
    "labels = []\n",
    "\n",
    "for i in range(0, df.shape[0] - N_TIME_STEPS, step):  \n",
    "    mag = df['rawData'].values[i: i + N_TIME_STEPS]\n",
    "    hr = df['hr'].values[i: i + N_TIME_STEPS]\n",
    "    segement = np.column_stack((mag, hr))  # Stack acceleration and heart rate features horizontally\n",
    "    label = stats.mode(df['label'][i: i + N_TIME_STEPS])[0][0]\n",
    "    segments.append(segement)\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert segments and labels to numpy arrays\n",
    "segments = np.asarray(segments, dtype=np.float32)\n",
    "labels = np.asarray(pd.get_dummies(labels), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1234, 125, 2) (1234, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(segments, labels, test_size=0.25, random_state=RANDOM_SEED)\n",
    "\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "# Assuming X_train and X_test have shape (samples, timesteps, 1)\n",
    "X_train = X_train.reshape(-1, N_TIME_STEPS, N_FEATURES)  # Reshape to include the feature dimension\n",
    "X_test = X_test.reshape(-1, N_TIME_STEPS, N_FEATURES)    # Reshape to include the feature dimension\n",
    "\n",
    "\n",
    "# Assuming X_train and X_test are already defined with shape (samples, timesteps, features)\n",
    "\n",
    "# Assuming X_train_acceleration and X_train_heart_rate are defined with shape (samples, timesteps, 1)\n",
    "X_train_acceleration = X_train[:, :, 0].reshape(-1, N_TIME_STEPS, 1)  # Reshape to include timestep dimension\n",
    "X_train_heart_rate = X_train[:, :, 1].reshape(-1, N_TIME_STEPS, 1)    # Reshape to include timestep dimension\n",
    "\n",
    "# Similarly for X_test data\n",
    "X_test_acceleration = X_test[:, :, 0].reshape(-1, N_TIME_STEPS, 1)    # Reshape to include timestep dimension\n",
    "X_test_heart_rate = X_test[:, :, 1].reshape(-1, N_TIME_STEPS, 1)      # Reshape to include timestep dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_heart_rate (1234, 125, 1)\n",
      "X_test_acceleration (1234, 125, 1)\n",
      "X_train_heart_rate (3701, 125, 1)\n",
      "X_test (1234, 125, 2)\n",
      "X_train (3701, 125, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test_heart_rate\",X_test_heart_rate.shape)\n",
    "print(\"X_test_acceleration\",X_test_acceleration.shape)\n",
    "print(\"X_train_heart_rate\",X_train_heart_rate.shape)\n",
    "print(\"X_test\",X_test.shape)\n",
    "print(\"X_train\",X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Test Shape =  (1234, 125, 2) Y Test Shape = (1234, 3)\n",
      "X_train (3701, 125, 2)\n",
      "X_test (1234, 125, 2)\n",
      "X_train_heart_rate (3701, 125, 1)\n",
      "X_train_acceleration (3701, 125, 1)\n",
      "X_train_heart_rate (3701, 125, 1)\n",
      "X_test_acceleration (1234, 125, 1)\n",
      "X_test_heart_rate (1234, 125, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X Test Shape = \", X_test.shape, \"Y Test Shape =\", y_test.shape)\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"X_train_heart_rate\", X_train_heart_rate.shape)\n",
    "print(\"X_train_acceleration\", X_train_acceleration.shape)\n",
    "print(\"X_train_heart_rate\", X_train_heart_rate.shape)\n",
    "print(\"X_test_acceleration\", X_test_acceleration.shape)\n",
    "print(\"X_test_heart_rate\", X_test_heart_rate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ReduceLROnPlateau callback\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, min_lr=0.0001, verbose=1)\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        \"../Models/model.h5\", save_best_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    reduce_lr,  # Include ReduceLROnPlateau callback\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_acceleration: (3701, 125, 1)\n",
      "Shape of X_train_heart_rate: (3701, 125, 1)\n",
      "Shape of X_test_acceleration: (1234, 125, 1)\n",
      "Shape of X_test_heart_rate: (1234, 125, 1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-cdb73014a57d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Iterate over each layer in the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Print the layer name and input shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Layer:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Permute\n",
    "\n",
    "# Assuming X_train_acceleratio                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        n, X_train_heart_rate, X_test_acceleration, X_test_heart_rate are defined\n",
    "# Assuming model is already defined\n",
    "\n",
    "# Check the shape of input data\n",
    "print(\"Shape of X_train_acceleration:\", X_train_acceleration.shape)\n",
    "print(\"Shape of X_train_heart_rate:\", X_train_heart_rate.shape)\n",
    "print(\"Shape of X_test_acceleration:\", X_test_acceleration.shape)\n",
    "print(\"Shape of X_test_heart_rate:\", X_test_heart_rate.shape)\n",
    "\n",
    "# Iterate over each layer in the model\n",
    "for layer in model.layers:\n",
    "    # Print the layer name and input shape\n",
    "    print(\"Layer:\", layer.name)\n",
    "    print(\"Input shape expected by the layer:\", layer.input_shape)\n",
    "\n",
    "# Check the input shapes expected by permute_26 layer\n",
    "permute_26_layer = None\n",
    "for layer in model.layers:\n",
    "    if layer.name == 'permute_26':\n",
    "        permute_26_layer = layer\n",
    "        break\n",
    "\n",
    "if permute_26_layer is not None:\n",
    "    print(\"Input shape expected by permute_26 layer:\", permute_26_layer.input_shape)\n",
    "else:\n",
    "    print(\"permute_26 layer not found in the model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of attention_acceleration: (None, 128)\n",
      "Shape of attention_heart_rate: (None, 128)\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_acceleration (InputLayer) [(None, 125, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_heart_rate (InputLayer)   [(None, 125, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute_13 (Permute)            (None, 125, 1)       0           input_acceleration[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "permute_14 (Permute)            (None, 125, 1)       0           input_heart_rate[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 125, 64)      256         permute_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 125, 64)      256         permute_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 125, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 125, 64)      256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 125, 64)      0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 125, 64)      0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 62, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 62, 64)       0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 62, 128)      24704       max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 62, 128)      24704       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 62, 128)      512         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 62, 128)      512         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 62, 128)      0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 62, 128)      0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 31, 128)      0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 31, 128)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 31, 256)      98560       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 31, 256)      98560       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 31, 256)      1024        conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 31, 256)      1024        conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 31, 256)      0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 31, 256)      0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 15, 256)      0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 15, 256)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_13 (Bidirectional (None, 15, 256)      394240      max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional (None, 15, 256)      394240      max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_27 (LSTM)                  (None, 128)          197120      bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_29 (LSTM)                  (None, 128)          197120      bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 128)          0           lstm_27[0][0]                    \n",
      "                                                                 lstm_27[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_13 (Attention)        (None, 128)          0           lstm_29[0][0]                    \n",
      "                                                                 lstm_29[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 128, 1)       0           attention_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 128, 1)       0           attention_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_12 (Multiply)          (None, 128, 128)     0           lstm_27[0][0]                    \n",
      "                                                                 reshape_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_13 (Multiply)          (None, 128, 128)     0           lstm_29[0][0]                    \n",
      "                                                                 reshape_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 128, 256)     0           multiply_12[0][0]                \n",
      "                                                                 multiply_13[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 128, 128)     32896       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128, 128)     0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 128, 128)     16512       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 128, 3)       387         dense_26[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,483,139\n",
      "Trainable params: 1,481,347\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#BUILD MODEL\n",
    "\n",
    "# Define regularization strength\n",
    "l2_regularizer_strength = 0.001\n",
    "\n",
    "# Assuming X_train is your input data with shape (3701, 125, 1)\n",
    "row, features = X_train_acceleration.shape[1:]\n",
    "\n",
    "# Define input shapes for each modality\n",
    "input_acceleration = Input(shape=(row, features), name='input_acceleration')\n",
    "input_heart_rate = Input(shape=(row, features), name='input_heart_rate')\n",
    "\n",
    "# Your code for CNN and LSTM branches\n",
    "\n",
    "\n",
    "# LSTM branch for acceleration\n",
    "trans_read_acceleration = lstm_pipe(Permute(dims=(1, 2))(input_acceleration), row_hidden, col_hidden)\n",
    "\n",
    "# LSTM branch for heart rate\n",
    "trans_read_heart_rate = lstm_pipe(Permute(dims=(1, 2))(input_heart_rate), row_hidden, col_hidden)\n",
    "\n",
    "\n",
    "# Attention Mechanisms\n",
    "\n",
    "# Attention Mechanism for acceleration\n",
    "attention_acceleration = Attention()([trans_read_acceleration, trans_read_acceleration])\n",
    "\n",
    "# Attention Mechanism for heart rate\n",
    "attention_heart_rate = Attention()([trans_read_heart_rate, trans_read_heart_rate])\n",
    "\n",
    "# Print the shapes of attention mechanisms using eager execution\n",
    "print(\"Shape of attention_acceleration:\", attention_acceleration.shape)\n",
    "print(\"Shape of attention_heart_rate:\", attention_heart_rate.shape)\n",
    "\n",
    "\n",
    "# Modality-specific attention weights\n",
    "\n",
    "# Reshape attention weights to match the shape of trans_read_acceleration and trans_read_heart_rate\n",
    "reshaped_attention_weights_acceleration = Reshape((-1, 1))(attention_acceleration)\n",
    "reshaped_attention_weights_heart_rate = Reshape((-1, 1))(attention_heart_rate)\n",
    "\n",
    "# Apply attention weights to features\n",
    "weighted_features_acceleration = Multiply()([trans_read_acceleration, reshaped_attention_weights_acceleration])\n",
    "weighted_features_heart_rate = Multiply()([trans_read_heart_rate, reshaped_attention_weights_heart_rate])\n",
    "\n",
    "# Concatenate attention-weighted features\n",
    "concatenated_features = Concatenate()([weighted_features_acceleration, weighted_features_heart_rate])\n",
    "\n",
    "# Your code for dense layers, prediction layer, and model compilation\n",
    "\n",
    "# Define your layer with L2 regularization\n",
    "layer = Dense(units=64, kernel_regularizer=regularizers.l2(0.01))\n",
    "\n",
    "# Dense layers\n",
    "dense_output = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_regularizer_strength))(concatenated_features)\n",
    "dense_output = Dropout(0.6)(dense_output)\n",
    "dense_output = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_regularizer_strength))(dense_output)\n",
    "\n",
    "epsilon_value = 1e-9  # Example epsilon value, you can adjust this value\n",
    "\n",
    "# Define the learning rate\n",
    "learning_rate = 0.001  # Example learning rate, you can adjust this value\n",
    "\n",
    "# Create an instance of the Adam optimizer with the desired learning rate\n",
    "optimizer = Adam(learning_rate=learning_rate,epsilon=epsilon_value)\n",
    "\n",
    "# Prediction layer\n",
    "prediction = Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(l2_regularizer_strength))(dense_output)\n",
    "\n",
    "# Define model with modality-specific inputs\n",
    "model = Model(inputs=[input_acceleration, input_heart_rate], outputs=prediction)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available\n",
      "TensorFlow GPU support:  \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if GPU is available and visible to TensorFlow\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    print(\"GPU is available\")\n",
    "    # Restrict TensorFlow to only allocate memory on demand\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "else:\n",
    "    print(\"No GPU available\")\n",
    "\n",
    "# Check TensorFlow's GPU usage\n",
    "print(\"TensorFlow GPU support: \", tf.test.gpu_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='../Results/model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_acceleration (3701, 125, 1)\n",
      "X_train_heart_rate (3701, 125, 1)\n",
      "X_test_acceleration (1234, 125, 1)\n",
      "X_test_heart_rate (1234, 125, 1)\n",
      "input_acceleration (None, 125, 1)\n",
      "input_heart_rate (None, 125, 1)\n",
      "prediction (None, 128, 3)\n",
      "dense_output (None, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_acceleration\",X_train_acceleration.shape)\n",
    "print(\"X_train_heart_rate\", X_train_heart_rate.shape)\n",
    "print(\"X_test_acceleration\", X_test_acceleration.shape)\n",
    "print(\"X_test_heart_rate\", X_test_heart_rate.shape)\n",
    "print(\"input_acceleration\", input_acceleration.shape)\n",
    "print(\"input_heart_rate\", input_heart_rate.shape)\n",
    "print(\"prediction\", prediction.shape)\n",
    "print(\"dense_output\", dense_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_reshaped: (3701, 128, 3)\n",
      "Shape of y_test_reshaped: (1234, 128, 3)\n",
      "Fold 1/5\n",
      "Epoch 1/25\n",
      "370/370 [==============================] - 42s 84ms/step - loss: 0.9762 - accuracy: 0.6864 - val_loss: 0.5700 - val_accuracy: 0.8206\n",
      "Epoch 2/25\n",
      "370/370 [==============================] - 25s 69ms/step - loss: 0.6641 - accuracy: 0.7741 - val_loss: 0.5282 - val_accuracy: 0.8262\n",
      "Epoch 3/25\n",
      "370/370 [==============================] - 25s 68ms/step - loss: 0.5799 - accuracy: 0.8063 - val_loss: 0.4161 - val_accuracy: 0.8702\n",
      "Epoch 4/25\n",
      "370/370 [==============================] - 25s 69ms/step - loss: 0.5382 - accuracy: 0.8048 - val_loss: 0.5463 - val_accuracy: 0.8150\n",
      "Epoch 5/25\n",
      "370/370 [==============================] - 25s 68ms/step - loss: 0.5160 - accuracy: 0.7985 - val_loss: 0.4385 - val_accuracy: 0.8576\n",
      "Epoch 6/25\n",
      "370/370 [==============================] - 25s 68ms/step - loss: 0.4760 - accuracy: 0.8320 - val_loss: 0.3646 - val_accuracy: 0.8734\n",
      "Epoch 7/25\n",
      "370/370 [==============================] - 29s 78ms/step - loss: 0.4656 - accuracy: 0.8245 - val_loss: 0.4490 - val_accuracy: 0.8215\n",
      "Epoch 8/25\n",
      "370/370 [==============================] - 32s 86ms/step - loss: 0.4482 - accuracy: 0.8394 - val_loss: 0.4781 - val_accuracy: 0.8446\n",
      "Epoch 9/25\n",
      "370/370 [==============================] - 32s 86ms/step - loss: 0.4486 - accuracy: 0.8310 - val_loss: 0.6083 - val_accuracy: 0.7350\n",
      "Epoch 10/25\n",
      "370/370 [==============================] - 34s 92ms/step - loss: 0.4936 - accuracy: 0.8089 - val_loss: 0.3980 - val_accuracy: 0.8564\n",
      "Epoch 11/25\n",
      "370/370 [==============================] - 34s 91ms/step - loss: 0.4483 - accuracy: 0.8242 - val_loss: 0.3858 - val_accuracy: 0.8632\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 12/25\n",
      "370/370 [==============================] - 26s 71ms/step - loss: 0.3799 - accuracy: 0.8643 - val_loss: 0.5506 - val_accuracy: 0.7747\n",
      "Epoch 13/25\n",
      "370/370 [==============================] - 26s 72ms/step - loss: 0.3580 - accuracy: 0.8658 - val_loss: 0.3041 - val_accuracy: 0.8986\n",
      "Epoch 14/25\n",
      "370/370 [==============================] - 26s 71ms/step - loss: 0.3700 - accuracy: 0.8655 - val_loss: 0.4889 - val_accuracy: 0.8033\n",
      "Epoch 15/25\n",
      "370/370 [==============================] - 27s 72ms/step - loss: 0.3593 - accuracy: 0.8700 - val_loss: 0.3153 - val_accuracy: 0.8823\n",
      "Epoch 16/25\n",
      "370/370 [==============================] - 26s 71ms/step - loss: 0.3699 - accuracy: 0.8582 - val_loss: 0.3132 - val_accuracy: 0.8941\n",
      "Epoch 17/25\n",
      "370/370 [==============================] - 26s 71ms/step - loss: 0.3717 - accuracy: 0.8661 - val_loss: 0.3873 - val_accuracy: 0.8611\n",
      "Epoch 18/25\n",
      "370/370 [==============================] - 26s 71ms/step - loss: 0.3649 - accuracy: 0.8704 - val_loss: 0.4557 - val_accuracy: 0.8254\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 19/25\n",
      "370/370 [==============================] - 26s 71ms/step - loss: 0.3230 - accuracy: 0.8901 - val_loss: 0.3184 - val_accuracy: 0.8954\n",
      "Epoch 20/25\n",
      "370/370 [==============================] - 26s 72ms/step - loss: 0.3117 - accuracy: 0.8941 - val_loss: 0.2890 - val_accuracy: 0.8992\n",
      "Epoch 21/25\n",
      "370/370 [==============================] - 28s 77ms/step - loss: 0.3535 - accuracy: 0.8709 - val_loss: 0.3687 - val_accuracy: 0.8568\n",
      "Epoch 22/25\n",
      "370/370 [==============================] - 28s 76ms/step - loss: 0.2976 - accuracy: 0.8975 - val_loss: 0.3110 - val_accuracy: 0.8948\n",
      "Epoch 23/25\n",
      "370/370 [==============================] - 33s 89ms/step - loss: 0.3385 - accuracy: 0.8802 - val_loss: 0.5101 - val_accuracy: 0.7905\n",
      "Epoch 24/25\n",
      "370/370 [==============================] - 26s 71ms/step - loss: 0.3021 - accuracy: 0.8901 - val_loss: 0.4321 - val_accuracy: 0.8315\n",
      "Epoch 25/25\n",
      "370/370 [==============================] - 28s 75ms/step - loss: 0.3165 - accuracy: 0.8860 - val_loss: 0.3225 - val_accuracy: 0.8784\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 0.3025 - accuracy: 0.8825\n",
      "Test Loss: 0.30253535509109497\n",
      "Test Accuracy: 0.8824856877326965\n",
      "Fold 2/5\n",
      "Epoch 1/25\n",
      "371/371 [==============================] - 37s 82ms/step - loss: 0.3303 - accuracy: 0.8870 - val_loss: 0.3508 - val_accuracy: 0.8845\n",
      "Epoch 2/25\n",
      "371/371 [==============================] - 28s 76ms/step - loss: 0.3013 - accuracy: 0.8984 - val_loss: 0.3592 - val_accuracy: 0.8736\n",
      "Epoch 3/25\n",
      "371/371 [==============================] - 37s 100ms/step - loss: 0.2932 - accuracy: 0.8974 - val_loss: 0.2817 - val_accuracy: 0.9003\n",
      "Epoch 4/25\n",
      "371/371 [==============================] - 37s 100ms/step - loss: 0.3185 - accuracy: 0.8886 - val_loss: 0.3502 - val_accuracy: 0.8868\n",
      "Epoch 5/25\n",
      "371/371 [==============================] - 33s 89ms/step - loss: 0.3036 - accuracy: 0.8922 - val_loss: 0.3426 - val_accuracy: 0.8780\n",
      "Epoch 6/25\n",
      "371/371 [==============================] - 33s 89ms/step - loss: 0.2908 - accuracy: 0.8984 - val_loss: 0.3770 - val_accuracy: 0.8708\n",
      "Epoch 7/25\n",
      "371/371 [==============================] - 33s 88ms/step - loss: 0.2985 - accuracy: 0.8970 - val_loss: 0.4120 - val_accuracy: 0.8440\n",
      "Epoch 8/25\n",
      "371/371 [==============================] - 33s 89ms/step - loss: 0.2947 - accuracy: 0.8969 - val_loss: 0.3730 - val_accuracy: 0.8642\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "Epoch 9/25\n",
      "371/371 [==============================] - 28s 76ms/step - loss: 0.2897 - accuracy: 0.8964 - val_loss: 0.3566 - val_accuracy: 0.8695\n",
      "Epoch 10/25\n",
      "371/371 [==============================] - 32s 85ms/step - loss: 0.2772 - accuracy: 0.9046 - val_loss: 0.3552 - val_accuracy: 0.8755\n",
      "Epoch 11/25\n",
      "371/371 [==============================] - 31s 85ms/step - loss: 0.2924 - accuracy: 0.8949 - val_loss: 0.3544 - val_accuracy: 0.8844\n",
      "Epoch 12/25\n",
      "371/371 [==============================] - 35s 94ms/step - loss: 0.2853 - accuracy: 0.9011 - val_loss: 0.4002 - val_accuracy: 0.8531\n",
      "Epoch 13/25\n",
      "371/371 [==============================] - 33s 88ms/step - loss: 0.2831 - accuracy: 0.8996 - val_loss: 0.4160 - val_accuracy: 0.8248\n",
      "Epoch 14/25\n",
      "371/371 [==============================] - 32s 87ms/step - loss: 0.2941 - accuracy: 0.9043 - val_loss: 0.4448 - val_accuracy: 0.8101\n",
      "Epoch 15/25\n",
      "371/371 [==============================] - 33s 89ms/step - loss: 0.2934 - accuracy: 0.9014 - val_loss: 0.3453 - val_accuracy: 0.8794\n",
      "Epoch 16/25\n",
      "371/371 [==============================] - 33s 89ms/step - loss: 0.2996 - accuracy: 0.8991 - val_loss: 0.3812 - val_accuracy: 0.8563\n",
      "Epoch 17/25\n",
      "371/371 [==============================] - 38s 104ms/step - loss: 0.2699 - accuracy: 0.9075 - val_loss: 0.4556 - val_accuracy: 0.8147\n",
      "Epoch 18/25\n",
      "371/371 [==============================] - 33s 89ms/step - loss: 0.2671 - accuracy: 0.9123 - val_loss: 0.3661 - val_accuracy: 0.8711\n",
      "Epoch 19/25\n",
      "371/371 [==============================] - 38s 101ms/step - loss: 0.2626 - accuracy: 0.9145 - val_loss: 0.4961 - val_accuracy: 0.8010\n",
      "Epoch 20/25\n",
      "371/371 [==============================] - 33s 89ms/step - loss: 0.2644 - accuracy: 0.9111 - val_loss: 0.3597 - val_accuracy: 0.8683\n",
      "Epoch 21/25\n",
      "371/371 [==============================] - 34s 91ms/step - loss: 0.2724 - accuracy: 0.9055 - val_loss: 0.4249 - val_accuracy: 0.8352\n",
      "Epoch 22/25\n",
      "371/371 [==============================] - 41s 112ms/step - loss: 0.2593 - accuracy: 0.9129 - val_loss: 0.3915 - val_accuracy: 0.8564\n",
      "Epoch 23/25\n",
      "371/371 [==============================] - 35s 93ms/step - loss: 0.2572 - accuracy: 0.9169 - val_loss: 0.4146 - val_accuracy: 0.8427\n",
      "Epoch 24/25\n",
      "371/371 [==============================] - 31s 85ms/step - loss: 0.2599 - accuracy: 0.9183 - val_loss: 0.4034 - val_accuracy: 0.8486\n",
      "Epoch 25/25\n",
      "371/371 [==============================] - 32s 85ms/step - loss: 0.2636 - accuracy: 0.9089 - val_loss: 0.3725 - val_accuracy: 0.8677\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 0.3139 - accuracy: 0.8954 0s - loss: 0.3141 - accuracy: 0.89\n",
      "Test Loss: 0.31391197443008423\n",
      "Test Accuracy: 0.8953863978385925\n",
      "Fold 3/5\n",
      "Epoch 1/25\n",
      "371/371 [==============================] - 29s 77ms/step - loss: 0.2725 - accuracy: 0.9044 - val_loss: 0.4985 - val_accuracy: 0.8066\n",
      "Epoch 2/25\n",
      "371/371 [==============================] - 32s 87ms/step - loss: 0.2722 - accuracy: 0.9091 - val_loss: 0.2087 - val_accuracy: 0.9479\n",
      "Epoch 3/25\n",
      "371/371 [==============================] - 33s 90ms/step - loss: 0.2667 - accuracy: 0.9060 - val_loss: 0.4410 - val_accuracy: 0.8242\n",
      "Epoch 4/25\n",
      "371/371 [==============================] - 32s 87ms/step - loss: 0.2717 - accuracy: 0.9068 - val_loss: 0.3766 - val_accuracy: 0.8622\n",
      "Epoch 5/25\n",
      "371/371 [==============================] - 36s 97ms/step - loss: 0.2745 - accuracy: 0.9059 - val_loss: 0.3780 - val_accuracy: 0.8613\n",
      "Epoch 6/25\n",
      "371/371 [==============================] - 29s 77ms/step - loss: 0.2687 - accuracy: 0.9057 - val_loss: 0.2876 - val_accuracy: 0.9102\n",
      "Epoch 7/25\n",
      "371/371 [==============================] - 28s 74ms/step - loss: 0.2643 - accuracy: 0.9117 - val_loss: 0.3964 - val_accuracy: 0.8528\n",
      "Epoch 8/25\n",
      "371/371 [==============================] - 27s 73ms/step - loss: 0.2888 - accuracy: 0.9024 - val_loss: 0.3944 - val_accuracy: 0.8449\n",
      "Epoch 9/25\n",
      "371/371 [==============================] - 27s 73ms/step - loss: 0.2600 - accuracy: 0.9118 - val_loss: 0.4299 - val_accuracy: 0.8313\n",
      "Epoch 10/25\n",
      "371/371 [==============================] - 27s 73ms/step - loss: 0.2534 - accuracy: 0.9157 - val_loss: 0.3654 - val_accuracy: 0.8587\n",
      "Epoch 11/25\n",
      "371/371 [==============================] - 27s 74ms/step - loss: 0.2456 - accuracy: 0.9184 - val_loss: 0.3198 - val_accuracy: 0.8841\n",
      "Epoch 12/25\n",
      "371/371 [==============================] - 27s 73ms/step - loss: 0.2567 - accuracy: 0.9148 - val_loss: 0.2343 - val_accuracy: 0.9313\n",
      "Epoch 13/25\n",
      "371/371 [==============================] - 27s 73ms/step - loss: 0.2586 - accuracy: 0.9145 - val_loss: 0.3403 - val_accuracy: 0.8749\n",
      "Epoch 14/25\n",
      "371/371 [==============================] - 27s 72ms/step - loss: 0.2414 - accuracy: 0.9198 - val_loss: 0.3214 - val_accuracy: 0.8862\n",
      "Epoch 15/25\n",
      "371/371 [==============================] - 27s 74ms/step - loss: 0.2458 - accuracy: 0.9166 - val_loss: 0.4208 - val_accuracy: 0.8370\n",
      "Epoch 16/25\n",
      "371/371 [==============================] - 28s 74ms/step - loss: 0.2470 - accuracy: 0.9198 - val_loss: 0.4054 - val_accuracy: 0.8472\n",
      "Epoch 17/25\n",
      "371/371 [==============================] - 27s 73ms/step - loss: 0.2415 - accuracy: 0.9198 - val_loss: 0.2951 - val_accuracy: 0.8983\n",
      "Epoch 18/25\n",
      "371/371 [==============================] - 27s 72ms/step - loss: 0.2633 - accuracy: 0.9126 - val_loss: 0.2659 - val_accuracy: 0.9102\n",
      "Epoch 19/25\n",
      "371/371 [==============================] - 27s 72ms/step - loss: 0.2497 - accuracy: 0.9158 - val_loss: 0.2991 - val_accuracy: 0.8961\n",
      "Epoch 20/25\n",
      "371/371 [==============================] - 27s 73ms/step - loss: 0.2462 - accuracy: 0.9157 - val_loss: 0.3434 - val_accuracy: 0.8783\n",
      "Epoch 21/25\n",
      "371/371 [==============================] - 27s 73ms/step - loss: 0.2376 - accuracy: 0.9225 - val_loss: 0.2991 - val_accuracy: 0.8977\n",
      "Epoch 22/25\n",
      "371/371 [==============================] - 27s 73ms/step - loss: 0.2405 - accuracy: 0.9209 - val_loss: 0.2573 - val_accuracy: 0.9145\n",
      "Epoch 23/25\n",
      "371/371 [==============================] - 27s 73ms/step - loss: 0.2314 - accuracy: 0.9222 - val_loss: 0.3733 - val_accuracy: 0.8585\n",
      "Epoch 24/25\n",
      "371/371 [==============================] - 27s 73ms/step - loss: 0.2482 - accuracy: 0.9161 - val_loss: 0.3907 - val_accuracy: 0.8537\n",
      "Epoch 25/25\n",
      "371/371 [==============================] - 35s 95ms/step - loss: 0.2375 - accuracy: 0.9241 - val_loss: 0.2682 - val_accuracy: 0.9129\n",
      "24/24 [==============================] - 2s 69ms/step - loss: 0.2413 - accuracy: 0.9233 0s - loss: 0.2307 - accuracy: 0.\n",
      "Test Loss: 0.24127799272537231\n",
      "Test Accuracy: 0.9232791662216187\n",
      "Fold 4/5\n",
      "Epoch 1/25\n",
      "371/371 [==============================] - 38s 102ms/step - loss: 0.2490 - accuracy: 0.9174 - val_loss: 0.3837 - val_accuracy: 0.8609\n",
      "Epoch 2/25\n",
      "371/371 [==============================] - 27s 72ms/step - loss: 0.2377 - accuracy: 0.9248 - val_loss: 0.2855 - val_accuracy: 0.9011\n",
      "Epoch 3/25\n",
      "371/371 [==============================] - 27s 73ms/step - loss: 0.2253 - accuracy: 0.9267 - val_loss: 0.2520 - val_accuracy: 0.9227\n",
      "Epoch 4/25\n",
      "371/371 [==============================] - 27s 73ms/step - loss: 0.2432 - accuracy: 0.9198 - val_loss: 0.3031 - val_accuracy: 0.8989\n",
      "Epoch 5/25\n",
      "371/371 [==============================] - 27s 74ms/step - loss: 0.2594 - accuracy: 0.9151 - val_loss: 0.3343 - val_accuracy: 0.8760\n",
      "Epoch 6/25\n",
      "371/371 [==============================] - 27s 73ms/step - loss: 0.2468 - accuracy: 0.9205 - val_loss: 0.3926 - val_accuracy: 0.8510\n",
      "Epoch 7/25\n",
      "371/371 [==============================] - 27s 73ms/step - loss: 0.2280 - accuracy: 0.9275 - val_loss: 0.3340 - val_accuracy: 0.8756\n",
      "Epoch 8/25\n",
      "371/371 [==============================] - 27s 73ms/step - loss: 0.2195 - accuracy: 0.9301 - val_loss: 0.3186 - val_accuracy: 0.8900\n",
      "Epoch 9/25\n",
      "371/371 [==============================] - 27s 74ms/step - loss: 0.2320 - accuracy: 0.9264 - val_loss: 0.5476 - val_accuracy: 0.7945\n",
      "Epoch 10/25\n",
      "371/371 [==============================] - 27s 73ms/step - loss: 0.2214 - accuracy: 0.9300 - val_loss: 0.2976 - val_accuracy: 0.8976\n",
      "Epoch 11/25\n",
      "371/371 [==============================] - 27s 73ms/step - loss: 0.2257 - accuracy: 0.9282 - val_loss: 0.2833 - val_accuracy: 0.9097\n",
      "Epoch 12/25\n",
      "371/371 [==============================] - 29s 78ms/step - loss: 0.2370 - accuracy: 0.9275 - val_loss: 0.2541 - val_accuracy: 0.9276\n",
      "Epoch 13/25\n",
      "371/371 [==============================] - 33s 88ms/step - loss: 0.2337 - accuracy: 0.9220 - val_loss: 0.3575 - val_accuracy: 0.8803\n",
      "Epoch 14/25\n",
      "371/371 [==============================] - 31s 84ms/step - loss: 0.2147 - accuracy: 0.9327 - val_loss: 0.4828 - val_accuracy: 0.8093\n",
      "Epoch 15/25\n",
      "371/371 [==============================] - 35s 94ms/step - loss: 0.2243 - accuracy: 0.9268 - val_loss: 0.2601 - val_accuracy: 0.9136\n",
      "Epoch 16/25\n",
      "371/371 [==============================] - 32s 87ms/step - loss: 0.2306 - accuracy: 0.9252 - val_loss: 0.4922 - val_accuracy: 0.8157\n",
      "Epoch 17/25\n",
      "371/371 [==============================] - 29s 79ms/step - loss: 0.2339 - accuracy: 0.9278 - val_loss: 0.2772 - val_accuracy: 0.9064\n",
      "Epoch 18/25\n",
      "371/371 [==============================] - 29s 78ms/step - loss: 0.2324 - accuracy: 0.9244 - val_loss: 0.4772 - val_accuracy: 0.8146\n",
      "Epoch 19/25\n",
      "371/371 [==============================] - 29s 79ms/step - loss: 0.2250 - accuracy: 0.9250 - val_loss: 0.4003 - val_accuracy: 0.8561\n",
      "Epoch 20/25\n",
      "371/371 [==============================] - 30s 80ms/step - loss: 0.2129 - accuracy: 0.9312 - val_loss: 0.2647 - val_accuracy: 0.9200\n",
      "Epoch 21/25\n",
      "371/371 [==============================] - 30s 81ms/step - loss: 0.2126 - accuracy: 0.9320 - val_loss: 0.3984 - val_accuracy: 0.8578\n",
      "Epoch 22/25\n",
      "371/371 [==============================] - 29s 79ms/step - loss: 0.2162 - accuracy: 0.9299 - val_loss: 0.6254 - val_accuracy: 0.7693\n",
      "Epoch 23/25\n",
      "371/371 [==============================] - 28s 76ms/step - loss: 0.2056 - accuracy: 0.9339 - val_loss: 0.4963 - val_accuracy: 0.8214\n",
      "Epoch 24/25\n",
      "371/371 [==============================] - 29s 79ms/step - loss: 0.2142 - accuracy: 0.9325 - val_loss: 0.2960 - val_accuracy: 0.9059\n",
      "Epoch 25/25\n",
      "371/371 [==============================] - 29s 78ms/step - loss: 0.2126 - accuracy: 0.9328 - val_loss: 0.4190 - val_accuracy: 0.8417\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.3677 - accuracy: 0.8531\n",
      "Test Loss: 0.3676845133304596\n",
      "Test Accuracy: 0.8531355857849121\n",
      "Fold 5/5\n",
      "Epoch 1/25\n",
      "371/371 [==============================] - 29s 78ms/step - loss: 0.2243 - accuracy: 0.9259 - val_loss: 0.1519 - val_accuracy: 0.9576\n",
      "Epoch 2/25\n",
      "371/371 [==============================] - 28s 76ms/step - loss: 0.2266 - accuracy: 0.9275 - val_loss: 0.2983 - val_accuracy: 0.8904\n",
      "Epoch 3/25\n",
      "371/371 [==============================] - 29s 77ms/step - loss: 0.2151 - accuracy: 0.9300 - val_loss: 0.3004 - val_accuracy: 0.8815\n",
      "Epoch 4/25\n",
      "371/371 [==============================] - 29s 79ms/step - loss: 0.2254 - accuracy: 0.9298 - val_loss: 0.2339 - val_accuracy: 0.9211\n",
      "Epoch 5/25\n",
      "371/371 [==============================] - 29s 79ms/step - loss: 0.2476 - accuracy: 0.9245 - val_loss: 0.3814 - val_accuracy: 0.8485\n",
      "Epoch 6/25\n",
      "371/371 [==============================] - 29s 79ms/step - loss: 0.2287 - accuracy: 0.9299 - val_loss: 0.1810 - val_accuracy: 0.9446\n",
      "Epoch 7/25\n",
      "371/371 [==============================] - 29s 79ms/step - loss: 0.2178 - accuracy: 0.9359 - val_loss: 0.2599 - val_accuracy: 0.9047\n",
      "Epoch 8/25\n",
      "371/371 [==============================] - 29s 79ms/step - loss: 0.2267 - accuracy: 0.9288 - val_loss: 0.4244 - val_accuracy: 0.8413\n",
      "Epoch 9/25\n",
      "371/371 [==============================] - 30s 80ms/step - loss: 0.2300 - accuracy: 0.9248 - val_loss: 0.1596 - val_accuracy: 0.9552\n",
      "Epoch 10/25\n",
      "371/371 [==============================] - 30s 80ms/step - loss: 0.2301 - accuracy: 0.9271 - val_loss: 0.1574 - val_accuracy: 0.9560\n",
      "Epoch 11/25\n",
      "371/371 [==============================] - 29s 79ms/step - loss: 0.2352 - accuracy: 0.9258 - val_loss: 0.4265 - val_accuracy: 0.8384\n",
      "Epoch 12/25\n",
      "371/371 [==============================] - 30s 80ms/step - loss: 0.2082 - accuracy: 0.9351 - val_loss: 0.3018 - val_accuracy: 0.8831\n",
      "Epoch 13/25\n",
      "371/371 [==============================] - 29s 79ms/step - loss: 0.2087 - accuracy: 0.9373 - val_loss: 0.1864 - val_accuracy: 0.9401\n",
      "Epoch 14/25\n",
      "371/371 [==============================] - 29s 79ms/step - loss: 0.2380 - accuracy: 0.9297 - val_loss: 0.2362 - val_accuracy: 0.9142\n",
      "Epoch 15/25\n",
      "371/371 [==============================] - 29s 78ms/step - loss: 0.2278 - accuracy: 0.9231 - val_loss: 0.1689 - val_accuracy: 0.9506\n",
      "Epoch 16/25\n",
      "371/371 [==============================] - 29s 78ms/step - loss: 0.2245 - accuracy: 0.9306 - val_loss: 0.2685 - val_accuracy: 0.8963\n",
      "Epoch 17/25\n",
      "371/371 [==============================] - 28s 77ms/step - loss: 0.2238 - accuracy: 0.9304 - val_loss: 0.3154 - val_accuracy: 0.8740\n",
      "Epoch 18/25\n",
      "371/371 [==============================] - 28s 77ms/step - loss: 0.2311 - accuracy: 0.9314 - val_loss: 0.1896 - val_accuracy: 0.9472\n",
      "Epoch 19/25\n",
      "371/371 [==============================] - 29s 78ms/step - loss: 0.1995 - accuracy: 0.9404 - val_loss: 0.2642 - val_accuracy: 0.9025\n",
      "Epoch 20/25\n",
      "371/371 [==============================] - 28s 76ms/step - loss: 0.2148 - accuracy: 0.9303 - val_loss: 0.3544 - val_accuracy: 0.8642\n",
      "Epoch 21/25\n",
      "371/371 [==============================] - 28s 77ms/step - loss: 0.2069 - accuracy: 0.9365 - val_loss: 0.2610 - val_accuracy: 0.8972\n",
      "Epoch 22/25\n",
      "371/371 [==============================] - 30s 80ms/step - loss: 0.2210 - accuracy: 0.9325 - val_loss: 0.2757 - val_accuracy: 0.8886\n",
      "Epoch 23/25\n",
      "371/371 [==============================] - 30s 81ms/step - loss: 0.2126 - accuracy: 0.9298 - val_loss: 0.4360 - val_accuracy: 0.8307\n",
      "Epoch 24/25\n",
      "371/371 [==============================] - 29s 79ms/step - loss: 0.2198 - accuracy: 0.9276 - val_loss: 0.3302 - val_accuracy: 0.8686\n",
      "Epoch 25/25\n",
      "371/371 [==============================] - 29s 79ms/step - loss: 0.2099 - accuracy: 0.9360 - val_loss: 0.4012 - val_accuracy: 0.8499\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 0.3382 - accuracy: 0.8674\n",
      "Test Loss: 0.33815571665763855\n",
      "Test Accuracy: 0.8673986196517944\n",
      "Average Test Loss: 0.31271311044692995\n",
      "Average Test Accuracy: 0.8843370914459229\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "# Reshape target data to match the sequence length of the model output\n",
    "y_train_reshaped = np.repeat(y_train[:, np.newaxis, :], 128, axis=1)\n",
    "y_test_reshaped = np.repeat(y_test[:, np.newaxis, :], 128, axis=1)\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(\"Shape of y_train_reshaped:\", y_train_reshaped.shape)\n",
    "print(\"Shape of y_test_reshaped:\", y_test_reshaped.shape)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Define the number of folds\n",
    "k = 5  # You can adjust this number as needed\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "kf = KFold(n_splits=k, shuffle=False)\n",
    "\n",
    "# Initialize lists to store evaluation metrics\n",
    "all_test_losses = []\n",
    "all_test_accuracies = []\n",
    "\n",
    "# Iterate over each fold\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X_train_acceleration)):\n",
    "    print(f\"Fold {fold + 1}/{k}\")\n",
    "\n",
    "    # Split data into train and test sets for this fold\n",
    "    X_fold_train_acceleration, X_fold_test_acceleration = X_train_acceleration[train_index], X_train_acceleration[test_index]\n",
    "    X_fold_train_heart_rate, X_fold_test_heart_rate = X_train_heart_rate[train_index], X_train_heart_rate[test_index]\n",
    "    y_fold_train, y_fold_test = y_train_reshaped[train_index], y_train_reshaped[test_index]\n",
    "\n",
    "    # Assuming you have already defined and compiled your model\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        [X_fold_train_acceleration, X_fold_train_heart_rate],  # input data\n",
    "        y_fold_train,  # target data\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=([X_fold_test_acceleration, X_fold_test_heart_rate], y_fold_test),\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the test set for this fold\n",
    "    test_loss, test_accuracy = model.evaluate([X_fold_test_acceleration, X_fold_test_heart_rate], y_fold_test)\n",
    "    print(\"Test Loss:\", test_loss)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "    # Store evaluation metrics for this fold\n",
    "    all_test_losses.append(test_loss)\n",
    "    all_test_accuracies.append(test_accuracy)\n",
    "\n",
    "# Calculate and print average performance across all folds\n",
    "avg_test_loss = np.mean(all_test_losses)\n",
    "avg_test_accuracy = np.mean(all_test_accuracies)\n",
    "print(\"Average Test Loss:\", avg_test_loss)\n",
    "print(\"Average Test Accuracy:\", avg_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"...Models/PAN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Confusion Matrix for Fold 1 :\n",
      "[[302.   0.   0.]\n",
      " [  3. 208.  79.]\n",
      " [ 24.   0. 125.]]\n",
      "\n",
      "Fold 2/5\n",
      "Confusion Matrix for Fold 2 :\n",
      "[[286.   0.   1.]\n",
      " [  3. 220.  91.]\n",
      " [ 21.   0. 118.]]\n",
      "\n",
      "Fold 3/5\n",
      "Confusion Matrix for Fold 3 :\n",
      "[[296.   0.   0.]\n",
      " [  6. 214.  86.]\n",
      " [ 18.   0. 119.]]\n",
      "\n",
      "Fold 4/5\n",
      "Confusion Matrix for Fold 4 :\n",
      "[[277.   0.   0.]\n",
      " [  2. 204.  94.]\n",
      " [ 22.   0. 141.]]\n",
      "\n",
      "Fold 5/5\n",
      "Confusion Matrix for Fold 5 :\n",
      "[[281.   1.   1.]\n",
      " [  2. 218.  72.]\n",
      " [ 22.   0. 143.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Initialize lists to store confusion matrices\n",
    "confusion_matrices = []\n",
    "\n",
    "# Define the number of decimal places to round to\n",
    "decimal_places = 0  # Adjust as needed\n",
    "\n",
    "# Iterate over each fold\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X_train_acceleration)):\n",
    "    print(f\"Fold {fold + 1}/{k}\")\n",
    "\n",
    "    # Split data into train and test sets for this fold\n",
    "    X_fold_train_acceleration, X_fold_test_acceleration = X_train_acceleration[train_index], X_train_acceleration[test_index]\n",
    "    X_fold_train_heart_rate, X_fold_test_heart_rate = X_train_heart_rate[train_index], X_train_heart_rate[test_index]\n",
    "    y_fold_train, y_fold_test = y_train_reshaped[train_index], y_train_reshaped[test_index]\n",
    "\n",
    "    # Evaluate the model on the test set for this fold\n",
    "    y_pred = model.predict([X_fold_test_acceleration, X_fold_test_heart_rate])\n",
    "    \n",
    "    # Convert predictions to classes\n",
    "    y_pred_classes = np.argmax(y_pred, axis=2)\n",
    "    y_true_classes = np.argmax(y_fold_test, axis=2)\n",
    "    \n",
    "    # Initialize an array to store the confusion matrices for each time step\n",
    "    cm_time_steps = []\n",
    "\n",
    "    # Calculate confusion matrix for each time step\n",
    "    for i in range(y_true_classes.shape[1]):\n",
    "        cm_time_step = confusion_matrix(y_true_classes[:, i], y_pred_classes[:, i], labels=range(3))  # assuming 3 classes\n",
    "        cm_time_steps.append(cm_time_step)\n",
    "    \n",
    "    # Average confusion matrix across time steps\n",
    "    cm_fold = np.mean(cm_time_steps, axis=0)\n",
    "    \n",
    "    # Round the confusion matrix to the specified number of decimal places\n",
    "    cm_fold_rounded = np.round(cm_fold, decimals=decimal_places)\n",
    "    \n",
    "    # Store confusion matrix for this fold\n",
    "    confusion_matrices.append(cm_fold_rounded)\n",
    "\n",
    "    # Print confusion matrix for this fold\n",
    "    print(\"Confusion Matrix for Fold\", fold + 1, \":\")\n",
    "    print(cm_fold_rounded)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.96     36224\n",
      "           1       1.00      0.75      0.85     37376\n",
      "           2       0.66      0.87      0.75     21120\n",
      "\n",
      "    accuracy                           0.87     94720\n",
      "   macro avg       0.86      0.87      0.85     94720\n",
      "weighted avg       0.89      0.87      0.87     94720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store true and predicted labels for all folds\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "\n",
    "# Evaluate the model on the test set for this fold\n",
    "y_pred = model.predict([X_fold_test_acceleration, X_fold_test_heart_rate])\n",
    "y_pred_classes = np.argmax(y_pred, axis=2)\n",
    "y_true_classes = np.argmax(y_fold_test, axis=2)\n",
    "\n",
    "# Store true and predicted labels for this fold\n",
    "all_true_labels.extend(y_true_classes.flatten())\n",
    "all_pred_labels.extend(y_pred_classes.flatten())\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_true_labels, all_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.8673986486486487\n",
      "Overall F1 Score: 0.8694112420519372\n",
      "Cohen's Kappa: 0.7992367219675061\n",
      "Matthews Correlation Coefficient (MCC): 0.8086519217809326\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, matthews_corrcoef\n",
    "\n",
    "# Calculate overall scores\n",
    "accuracy = accuracy_score(all_true_labels, all_pred_labels)\n",
    "f1 = f1_score(all_true_labels, all_pred_labels, average='weighted')  # Use weighted average for multiclass problems\n",
    "cohen_kappa = cohen_kappa_score(all_true_labels, all_pred_labels)\n",
    "mcc = matthews_corrcoef(all_true_labels, all_pred_labels)\n",
    "\n",
    "# Print overall scores\n",
    "print(\"Overall Accuracy:\", accuracy)\n",
    "print(\"Overall F1 Score:\", f1)\n",
    "print(\"Cohen's Kappa:\", cohen_kappa)\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Metrics:\n",
      "Sensitivity (TPR): [0.99293286 0.74657534 0.86666667]\n",
      "Specificity (TNR): [0.94748359 0.99776786 0.87304348]\n",
      "Precision (PPV): [0.92131148 0.99543379 0.66203704]\n",
      "Negative Predictive Value (NPV): [0.9954023  0.85796545 0.95801527]\n",
      "False Positive Rate (FPR): [0.05251641 0.00223214 0.12695652]\n",
      "False Negative Rate (FNR): [0.00706714 0.25342466 0.13333333]\n",
      "False Discovery Rate (FDR): [0.07868852 0.00456621 0.33796296]\n",
      "Overall Accuracy: [0.96486486 0.89864865 0.87162162]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate various metrics\n",
    "TP = np.diag(cm_fold_rounded)\n",
    "FP = cm_fold_rounded.sum(axis=0) - TP\n",
    "FN = cm_fold_rounded.sum(axis=1) - TP\n",
    "TN = cm_fold_rounded.sum() - (TP + FP + FN)\n",
    "\n",
    "# Calculate various metrics\n",
    "TPR = TP / (TP + FN)\n",
    "TNR = TN / (TN + FP)\n",
    "PPV = TP / (TP + FP)\n",
    "NPV = TN / (TN + FN)\n",
    "FPR = FP / (FP + TN)\n",
    "FNR = FN / (TP + FN)\n",
    "FDR = FP / (TP + FP)\n",
    "ACC = (TP + TN) / (TP + FP + FN + TN)\n",
    "\n",
    "# Display the metrics\n",
    "print(\"Overall Metrics:\")\n",
    "print(\"Sensitivity (TPR):\", TPR)\n",
    "print(\"Specificity (TNR):\", TNR)\n",
    "print(\"Precision (PPV):\", PPV)\n",
    "print(\"Negative Predictive Value (NPV):\", NPV)\n",
    "print(\"False Positive Rate (FPR):\", FPR)\n",
    "print(\"False Negative Rate (FNR):\", FNR)\n",
    "print(\"False Discovery Rate (FDR):\", FDR)\n",
    "print(\"Overall Accuracy:\", ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Metrics:\n",
      "Macro-average Sensitivity (TPR): 0.8687249571077443\n",
      "Macro-average Specificity (TNR): 0.9394316413417236\n",
      "Macro-average Precision (PPV): 0.8595941008004037\n",
      "Macro-average Negative Predictive Value (NPV): 0.9371276723606031\n",
      "Macro-average False Positive Rate (FPR): 0.06056835865827637\n",
      "Macro-average False Negative Rate (FNR): 0.13127504289225575\n",
      "Macro-average False Discovery Rate (FDR): 0.14040589919959634\n",
      "Macro-average Overall Accuracy: 0.9117117117117117\n",
      "\n",
      "Micro-average Sensitivity (TPR): 0.8675675675675676\n",
      "Micro-average Specificity (TNR): 0.9337837837837838\n",
      "Micro-average Precision (PPV): 0.8675675675675676\n",
      "Micro-average Negative Predictive Value (NPV): 0.9337837837837838\n",
      "Micro-average False Positive Rate (FPR): 0.06621621621621622\n",
      "Micro-average False Negative Rate (FNR): 0.13243243243243244\n",
      "Micro-average False Discovery Rate (FDR): 0.13243243243243244\n",
      "Micro-average Overall Accuracy: 0.9117117117117117\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall metrics\n",
    "macro_TPR = np.mean(TPR)\n",
    "macro_TNR = np.mean(TNR)\n",
    "macro_PPV = np.mean(PPV)\n",
    "macro_NPV = np.mean(NPV)\n",
    "macro_FPR = np.mean(FPR)\n",
    "macro_FNR = np.mean(FNR)\n",
    "macro_FDR = np.mean(FDR)\n",
    "macro_ACC = np.mean(ACC)\n",
    "\n",
    "micro_TPR = np.sum(TP) / np.sum(TP + FN)\n",
    "micro_TNR = np.sum(TN) / np.sum(TN + FP)\n",
    "micro_PPV = np.sum(TP) / np.sum(TP + FP)\n",
    "micro_NPV = np.sum(TN) / np.sum(TN + FN)\n",
    "micro_FPR = np.sum(FP) / np.sum(FP + TN)\n",
    "micro_FNR = np.sum(FN) / np.sum(TP + FN)\n",
    "micro_FDR = np.sum(FP) / np.sum(TP + FP)\n",
    "micro_ACC = np.sum(TP + TN) / np.sum(TP + FP + FN + TN)\n",
    "\n",
    "# Display the overall metrics\n",
    "print(\"Overall Metrics:\")\n",
    "print(\"Macro-average Sensitivity (TPR):\", macro_TPR)\n",
    "print(\"Macro-average Specificity (TNR):\", macro_TNR)\n",
    "print(\"Macro-average Precision (PPV):\", macro_PPV)\n",
    "print(\"Macro-average Negative Predictive Value (NPV):\", macro_NPV)\n",
    "print(\"Macro-average False Positive Rate (FPR):\", macro_FPR)\n",
    "print(\"Macro-average False Negative Rate (FNR):\", macro_FNR)\n",
    "print(\"Macro-average False Discovery Rate (FDR):\", macro_FDR)\n",
    "print(\"Macro-average Overall Accuracy:\", macro_ACC)\n",
    "print()\n",
    "print(\"Micro-average Sensitivity (TPR):\", micro_TPR)\n",
    "print(\"Micro-average Specificity (TNR):\", micro_TNR)\n",
    "print(\"Micro-average Precision (PPV):\", micro_PPV)\n",
    "print(\"Micro-average Negative Predictive Value (NPV):\", micro_NPV)\n",
    "print(\"Micro-average False Positive Rate (FPR):\", micro_FPR)\n",
    "print(\"Micro-average False Negative Rate (FNR):\", micro_FNR)\n",
    "print(\"Micro-average False Discovery Rate (FDR):\", micro_FDR)\n",
    "print(\"Micro-average Overall Accuracy:\", micro_ACC)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
